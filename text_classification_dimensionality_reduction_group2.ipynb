{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a48e7e5a",
   "metadata": {},
   "source": [
    "# Text Classification with Dimensionality Reduction  \n",
    "### *Airline Tweet Sentiment Analysis using TF-IDF, Naive Bayes, SVD, PCA & Logistic Regression*\n",
    "\n",
    "---\n",
    "\n",
    "## Group 2 Members\n",
    "**Albright Maduka Ifechukwude – 9053136**  \n",
    "**Abdullahi Abdirizak Mohamed – 9082466**  \n",
    "**Kamamo Lesley Wanjiku - 8984971**\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project explores the task of **binary text classification** using real-world airline customer tweets.  \n",
    "Building on the techniques outlined in our course materials, we implement a complete Natural Language Processing (NLP) workflow that transforms raw textual data into meaningful numerical features and analyzes how different models perform on sentiment classification.\n",
    "\n",
    "We focus on:\n",
    "- Converting tweets into TF-IDF feature vectors  \n",
    "- Reducing feature dimensionality using **SVD (TruncatedSVD)** and **PCA**  \n",
    "- Comparing baseline and advanced machine learning models  \n",
    "- Evaluating performance using confusion matrices and standard classification metrics  \n",
    "\n",
    "To align with the project requirements, we:\n",
    "- Select **two airlines** (United Airlines and Delta Airlines)  \n",
    "- Reduce the sentiment labels to **positive vs negative** (binary classification)  \n",
    "- Apply normalization and text preprocessing to clean noisy tweet data  \n",
    "- Train three models:\n",
    "  1. **Naive Bayes using TF-IDF** (baseline)  \n",
    "  2. **Logistic Regression using SVD-reduced features**  \n",
    "  3. **Logistic Regression using PCA-reduced features**  \n",
    "\n",
    "This introduction serves as the foundation for the detailed analysis, modeling, and evaluation presented in the following sections of the notebook.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c3f53",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries & Loading the Dataset\n",
    "\n",
    "In this section, we import all the necessary Python libraries for data processing, \n",
    "visualization, feature extraction, dimensionality reduction, and machine learning.\n",
    "\n",
    "We then load the `Tweets.csv` dataset, which contains airline customer tweets along \n",
    "with sentiment labels.  \n",
    "\n",
    "This dataset will be filtered later to match the binary classification requirement \n",
    "of the project (positive vs negative).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a12309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tweet_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "airline_sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "airline_sentiment_confidence",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "negativereason",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "negativereason_confidence",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "airline",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "airline_sentiment_gold",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "negativereason_gold",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "retweet_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tweet_coord",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tweet_created",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tweet_location",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "user_timezone",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "521b79e7-74ef-410a-bd95-b2bd44611c75",
       "rows": [
        [
         "0",
         "570306133677760513",
         "neutral",
         "1.0",
         null,
         null,
         "Virgin America",
         null,
         "cairdin",
         null,
         "0",
         "@VirginAmerica What @dhepburn said.",
         null,
         "2015-02-24 11:35:52 -0800",
         null,
         "Eastern Time (US & Canada)"
        ],
        [
         "1",
         "570301130888122368",
         "positive",
         "0.3486",
         null,
         "0.0",
         "Virgin America",
         null,
         "jnardino",
         null,
         "0",
         "@VirginAmerica plus you've added commercials to the experience... tacky.",
         null,
         "2015-02-24 11:15:59 -0800",
         null,
         "Pacific Time (US & Canada)"
        ],
        [
         "2",
         "570301083672813571",
         "neutral",
         "0.6837",
         null,
         null,
         "Virgin America",
         null,
         "yvonnalynn",
         null,
         "0",
         "@VirginAmerica I didn't today... Must mean I need to take another trip!",
         null,
         "2015-02-24 11:15:48 -0800",
         "Lets Play",
         "Central Time (US & Canada)"
        ],
        [
         "3",
         "570301031407624196",
         "negative",
         "1.0",
         "Bad Flight",
         "0.7033",
         "Virgin America",
         null,
         "jnardino",
         null,
         "0",
         "@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse",
         null,
         "2015-02-24 11:15:36 -0800",
         null,
         "Pacific Time (US & Canada)"
        ],
        [
         "4",
         "570300817074462722",
         "negative",
         "1.0",
         "Can't Tell",
         "1.0",
         "Virgin America",
         null,
         "jnardino",
         null,
         "0",
         "@VirginAmerica and it's a really big bad thing about it",
         null,
         "2015-02-24 11:14:45 -0800",
         null,
         "Pacific Time (US & Canada)"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0. Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, classification_report\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load CSV\n",
    "df = pd.read_csv(\"data\\Tweets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5961e01c",
   "metadata": {},
   "source": [
    "## 2. Dataset Filtering: Selecting Airlines & Creating Multi-Class Labels\n",
    "\n",
    "Following the project requirement for sentiment classification, we:\n",
    "\n",
    "- Select **two airlines**: United and Delta.\n",
    "- Keep **positive**, **negative**, and **neutral** tweets.\n",
    "- Convert sentiment into a three-class label:\n",
    "  - **0 = Negative**\n",
    "  - **1 = Positive**\n",
    "  - **2 = Neutral**\n",
    "\n",
    "We also balance the dataset to approximately ~2,000 total tweets, ensuring that all \n",
    "three classes are equally represented.  \n",
    "This step prevents model bias and improves evaluation reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368490fe",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing & Normalization\n",
    "\n",
    "Raw tweets contain noise such as:\n",
    "- URLs  \n",
    "- Mentions (@username)  \n",
    "- Special characters & punctuation  \n",
    "- Random spacing  \n",
    "- Mixed casing  \n",
    "\n",
    "To prepare the data for TF-IDF and machine learning, we clean each tweet using a \n",
    "custom normalization function that:\n",
    "- Converts text to lowercase\n",
    "- Removes URLs and mentions\n",
    "- Removes non-alphabetical characters\n",
    "- Collapses extra spaces\n",
    "\n",
    "The resulting `clean_text` column is used for all subsequent modeling steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae946736",
   "metadata": {},
   "source": [
    "## 4. TF-IDF Feature Extraction\n",
    "\n",
    "TF-IDF (Term Frequency–Inverse Document Frequency) converts text into numerical vectors.  \n",
    "This representation emphasizes:\n",
    "- Words that appear frequently in an individual tweet, and  \n",
    "- Words that are rare across the entire dataset.\n",
    "\n",
    "We use:\n",
    "- Maximum vocabulary size: 5000 terms  \n",
    "- Unigrams + bigrams (1–2 word phrases)  \n",
    "- English stopword removal\n",
    "\n",
    "TF-IDF produces a **high-dimensional sparse matrix**, essential for the baseline Naive \n",
    "Bayes model and for dimensionality reduction (SVD & PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6b3eb",
   "metadata": {},
   "source": [
    "## 5. Model 1 — Naive Bayes (Baseline with TF-IDF)\n",
    "\n",
    "Naive Bayes is a common and effective baseline model for text classification.  \n",
    "It works well with TF-IDF because:\n",
    "- It assumes word independence (bag-of-words assumption)\n",
    "- It handles high-dimensional sparse features efficiently\n",
    "- It performs strongly on short text like tweets\n",
    "\n",
    "In this section, we:\n",
    "- Train a Multinomial Naive Bayes classifier\n",
    "- Predict sentiment for the test set\n",
    "- Generate a confusion matrix and evaluation metrics\n",
    "\n",
    "This provides a foundation for comparing models with dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf863a",
   "metadata": {},
   "source": [
    "## 6. Dimensionality Reduction using SVD (TruncatedSVD)\n",
    "\n",
    "TF-IDF generates thousands of features, many of which are redundant or noisy.  \n",
    "We apply **TruncatedSVD**, also known as Latent Semantic Analysis (LSA), to reduce \n",
    "the dimensionality to about 100 components.\n",
    "\n",
    "Why SVD?\n",
    "- Produces **dense semantic features**\n",
    "- Captures latent topics in the text\n",
    "- Improves model performance and training speed\n",
    "- Works directly on sparse TF-IDF matrices\n",
    "\n",
    "We also visualize the **explained variance curve** to show how much information \n",
    "each SVD component retains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4fc70",
   "metadata": {},
   "source": [
    "## 7. Model 2 — Logistic Regression with SVD-Reduced Features\n",
    "\n",
    "After reducing TF-IDF using SVD, we train a **Logistic Regression** model on the \n",
    "dense, lower-dimensional feature set.\n",
    "\n",
    "Logistic Regression is:\n",
    "- Robust  \n",
    "- Interpretable  \n",
    "- Effective for binary classification  \n",
    "\n",
    "We evaluate the model using:\n",
    "- Confusion matrix  \n",
    "- Accuracy, precision, recall, F1-score  \n",
    "\n",
    "We later compare this model directly to the Naive Bayes baseline and PCA-reduced model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4519b974",
   "metadata": {},
   "source": [
    "## 8. Dimensionality Reduction using PCA\n",
    "\n",
    "Unlike SVD, **PCA requires dense, standardized data**, so we first convert TF-IDF \n",
    "to a dense array and apply standardization.\n",
    "\n",
    "We then reduce to the **same number of components as SVD** to ensure a fair \n",
    "comparison between the two dimensionality reduction techniques.\n",
    "\n",
    "We also visualize PCA's **explained variance curve**, which shows how much of the \n",
    "data's variance is preserved across components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b0495",
   "metadata": {},
   "source": [
    "## 9. Model 3 — Logistic Regression with PCA-Reduced Features\n",
    "\n",
    "We train another Logistic Regression model, this time using PCA-transformed features.\n",
    "\n",
    "This allows us to compare:\n",
    "- SVD vs PCA performance  \n",
    "- The effect of dimensionality reduction on classification accuracy  \n",
    "- Differences in semantic vs variance-based transformations of TF-IDF  \n",
    "\n",
    "As before, we evaluate using confusion matrices and standard metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd6c4e",
   "metadata": {},
   "source": [
    "## 10. Final Comparison of All Models\n",
    "\n",
    "We summarize and compare the performance of all three models:\n",
    "\n",
    "1. **Naive Bayes + TF-IDF** (baseline)  \n",
    "2. **Logistic Regression + SVD**  \n",
    "3. **Logistic Regression + PCA**\n",
    "\n",
    "We present:\n",
    "- A combined performance table  \n",
    "- Accuracy, precision, recall, and F1-scores  \n",
    "- A discussion of which model performs best  \n",
    "- Error analysis (FP/FN patterns)  \n",
    "- Insights on dimensionality reduction effectiveness\n",
    "\n",
    "This section forms the core of the presentation and final report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17f085",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "We reflect on the overall performance of the three models and highlight:\n",
    "\n",
    "- Which approach yields the highest accuracy  \n",
    "- Whether dimensionality reduction helps or hurts performance  \n",
    "- Which method (SVD or PCA) is more suitable for text data  \n",
    "- Strengths and weaknesses of each model  \n",
    "- Observations from Delta vs United sentiment trends  \n",
    "- Suggestions for future improvements (deep learning, more features, larger dataset)\n",
    "\n",
    "This conclusion ties together all analysis and supports the final presentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
